{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required libraries\n\nimport os\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom transformers import AutoTokenizer, AutoModel, AutoImageProcessor, ViTModel\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nfrom datasets import load_dataset\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nimport matplotlib.pyplot as plt\nfrom torchvision.transforms import Compose, Resize, ToTensor, Normalize\n\n\nfrom IPython.display import display\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the VQAv2 dataset from Hugging Face Hub\n# %%timeC\ndataset = load_dataset(\"HuggingFaceM4/VQAv2\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Access train, validation, and test sets\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\nval_dataset = dataset['validation']\n\nprint(train_dataset[0]['image'])\n\nimage=train_dataset[0]['image']\ndisplay(image)\nanswer=train_dataset[0]['answers']\nprint(answer)\n# If you need to ensure the image is in RGB mode\nimage = image.convert(\"RGB\")\n\ndef load_image(image):\n    return image.convert(\"RGB\")\n\ndef display_image(image):\n    display(image)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up environment\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenizer and Image Processor\nbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, dataset, tokenizer, image_processor, label_encoder):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.image_processor = image_processor\n        self.label_encoder = label_encoder\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        question = item['question']\n        answers = item['answers']\n        image = item['image'].convert(\"RGB\")\n\n        text_inputs = self.tokenizer(question, padding='max_length', truncation=True, return_tensors=\"pt\")\n        image_inputs = self.image_processor(images=[image], return_tensors=\"pt\")\n\n        text_inputs = {k: v.squeeze(0) for k, v in text_inputs.items()}\n        image_inputs = {k: v.squeeze(0) for k, v in image_inputs.items()}\n\n        # Convert answers to soft targets\n        answer_texts = [answer['answer'] for answer in answers]\n        encoded_labels = self.label_encoder.transform(answer_texts)\n        one_hot_labels = np.zeros((len(self.label_encoder.classes_)))\n        for encoded_label in encoded_labels:\n            one_hot_labels[encoded_label] += 1\n        one_hot_labels /= len(encoded_labels)  # Average the one-hot vectors\n\n        label = torch.tensor(one_hot_labels, dtype=torch.float)  # Convert to tensor\n\n        return {'text_inputs': text_inputs, 'image_inputs': image_inputs, 'labels': label}\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VQAModel(nn.Module):\n    def __init__(self, text_model_name=\"bert-base-cased\", image_model_name=\"google/vit-base-patch16-224\", num_answers=1000):\n        super(VQAModel, self).__init__()\n        self.text_model = AutoModel.from_pretrained(text_model_name)\n        self.image_model = ViTModel.from_pretrained(image_model_name)\n        self.text_fc = nn.Linear(self.text_model.config.hidden_size, 512)\n        self.image_fc = nn.Linear(self.image_model.config.hidden_size, 512)\n        self.classifier = nn.Linear(1024, num_answers)\n        \n    def forward(self, text_inputs, image_inputs):\n        text_outputs = self.text_model(**text_inputs).last_hidden_state[:, 0, :]  # CLS token\n        image_outputs = self.image_model(**image_inputs).last_hidden_state[:, 0, :]  # CLS token\n        text_features = self.text_fc(text_outputs)\n        image_features = self.image_fc(image_outputs)\n        combined_features = torch.cat((text_features, image_features), dim=1)\n        logits = self.classifier(combined_features)\n        return logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\nall_answer_texts = [answer['answer'] for example in train_dataset for answer in example['answers']]\nlabel_encoder.fit(all_answer_texts)\nrandom.seed(42)\nsubset_indices = random.sample(range(len(dataset['train'])), len(dataset['train']) // 4)\nsubset_train_dataset = Subset(VQADataset(dataset['train'], bert_tokenizer, image_processor, label_encoder), subset_indices)","metadata":{},"execution_count":null,"outputs":[]}]}