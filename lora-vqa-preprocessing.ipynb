{"cells":[{"cell_type":"code","execution_count":125,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-17T15:50:34.480876Z","iopub.status.busy":"2024-05-17T15:50:34.480155Z","iopub.status.idle":"2024-05-17T15:50:47.078010Z","shell.execute_reply":"2024-05-17T15:50:47.076721Z","shell.execute_reply.started":"2024-05-17T15:50:34.480846Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["# Install required libraries\n","!pip install transformers datasets torch peft -q\n","\n","import os\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from transformers import AutoTokenizer, AutoModel, AutoImageProcessor, ViTModel\n","from PIL import Image\n","from sklearn.preprocessing import LabelEncoder\n","from datasets import load_dataset\n","from peft import LoraConfig, get_peft_model\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n","\n","\n","from IPython.display import display\n","import random"]},{"cell_type":"code","execution_count":126,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-17T15:51:08.319645Z","iopub.status.busy":"2024-05-17T15:51:08.318812Z","iopub.status.idle":"2024-05-17T15:51:09.807349Z","shell.execute_reply":"2024-05-17T15:51:09.806546Z","shell.execute_reply.started":"2024-05-17T15:51:08.319605Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Repo card metadata block was not found. Setting CardData to empty.\n"]}],"source":["# %%time\n","# Load the VQAv2 dataset from Hugging Face Hub\n","dataset = load_dataset(\"HuggingFaceM4/VQAv2\")"]},{"cell_type":"code","execution_count":127,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:12.055690Z","iopub.status.busy":"2024-05-17T15:51:12.054878Z","iopub.status.idle":"2024-05-17T15:51:12.060187Z","shell.execute_reply":"2024-05-17T15:51:12.059181Z","shell.execute_reply.started":"2024-05-17T15:51:12.055660Z"},"trusted":true},"outputs":[],"source":["# Access train, validation, and test sets\n","train_dataset = dataset['train']\n","test_dataset = dataset['test']\n","val_dataset = dataset['validation']\n","\n","print(train_dataset[0])\n","image=train_dataset[0]['image']\n","display(image)\n","answer=train_dataset[0]['answers']\n","print(answer)\n","# If you need to ensure the image is in RGB mode\n","image = image.convert(\"RGB\")\n","\n","def load_image(image):\n","    return image.convert(\"RGB\")\n","\n","def display_image(image):\n","    display(image)"]},{"cell_type":"code","execution_count":128,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:13.584028Z","iopub.status.busy":"2024-05-17T15:51:13.583279Z","iopub.status.idle":"2024-05-17T15:51:13.973615Z","shell.execute_reply":"2024-05-17T15:51:13.972535Z","shell.execute_reply.started":"2024-05-17T15:51:13.583992Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# Tokenizer and Image Processor setup\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:15.229187Z","iopub.status.busy":"2024-05-17T15:51:15.228318Z","iopub.status.idle":"2024-05-17T15:51:15.237567Z","shell.execute_reply":"2024-05-17T15:51:15.236604Z","shell.execute_reply.started":"2024-05-17T15:51:15.229151Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# Custom Dataset class\n","class VQADataset(Dataset):\n","    def __init__(self, dataset, tokenizer, image_processor):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.image_processor = image_processor\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        item = self.dataset[idx]\n","        question = item['question']\n","        answers = item['answers']\n","        image = item['image'].convert(\"RGB\")\n","\n","        text_inputs = self.tokenizer(question, padding='max_length', truncation=True, return_tensors=\"pt\")\n","        image_inputs = self.image_processor(images=[image], return_tensors=\"pt\")\n","\n","        text_inputs = {k: v.squeeze(0) for k, v in text_inputs.items()}\n","        image_inputs = {k: v.squeeze(0) for k, v in image_inputs.items()}\n","        \n","        answer_texts = [answer['answer'] for answer in answers]\n","        encoded_labels = self.label_encoder.transform(answer_texts)\n","        one_hot_labels = np.zeros((len(self.label_encoder.classes_)))\n","        for encoded_label in encoded_labels:\n","            one_hot_labels[encoded_label] += 1\n","        one_hot_labels /= len(encoded_labels)  # Average the one-hot vectors\n","\n","        label = torch.tensor(one_hot_labels, dtype=torch.float)  # Convert to tensor\n","\n","        return {'text_inputs': text_inputs, 'image_inputs': image_inputs, 'labels': label}"]},{"cell_type":"code","execution_count":130,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:16.735561Z","iopub.status.busy":"2024-05-17T15:51:16.735220Z","iopub.status.idle":"2024-05-17T15:51:16.741124Z","shell.execute_reply":"2024-05-17T15:51:16.740143Z","shell.execute_reply.started":"2024-05-17T15:51:16.735535Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# Data preparation\n","label_encoder = LabelEncoder()\n","all_answer_texts = [answer['answer'] for example in train_dataset for answer in example['answers']]\n","label_encoder.fit(all_answer_texts)\n","train_dataset = Subset(VQADataset(dataset['train'], bert_tokenizer, image_processor), range(len(dataset['train']) // 4))\n","dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
