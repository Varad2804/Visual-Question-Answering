{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport time\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom transformers import AutoTokenizer, AutoModel, AutoImageProcessor, ViTModel\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom copy import deepcopy\nfrom transformers import Trainer, TrainingArguments\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T16:00:37.890841Z","iopub.execute_input":"2024-05-18T16:00:37.891295Z","iopub.status.idle":"2024-05-18T16:00:37.898722Z","shell.execute_reply.started":"2024-05-18T16:00:37.891265Z","shell.execute_reply":"2024-05-18T16:00:37.897620Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load the VQAv2 dataset from Hugging Face Hub\n# %%timeC\ndataset = load_dataset(\"HuggingFaceM4/VQAv2\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.idle":"2024-05-18T16:24:41.258429Z","shell.execute_reply.started":"2024-05-18T16:00:37.900927Z","shell.execute_reply":"2024-05-18T16:24:41.257663Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for HuggingFaceM4/VQAv2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/VQAv2\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8f67120dbfd4bdea7365e59f82d188c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/352 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"624a35c88255483ebc8ac4bc5f8f9cc5"}},"metadata":{}},{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c1917fcf24d4c1a919a0b41ae010067"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.49M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9db8a7fd41046adaddd71b1d80160dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.97M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ecd37fbdec54d54a91535253fb20165"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/21.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97430d50f17d45b591de07e4e3da0c35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/10.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dad3d6ea5b814c1db3ad61d5e2405483"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.5G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d25a5e74bb6e464b8d1958907b4bfa9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/6.65G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0ed9272aae144f680c8d8ec4cc78f46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.3G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122a6713a20a451e80c5972e21e0d532"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d2c1eaac9ce43ce93e2bf9b747c1a1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"722823d50b284c6282011604eaa69c39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating testdev split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7e8e54a5c4247e9984ce1505bf17e78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"030cba7b04dc4e8c9d86173980e08e48"}},"metadata":{}}]},{"cell_type":"code","source":"# Access train, validation, and test sets\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']\nval_dataset = dataset['validation']\n\nprint(train_dataset[0]['image'])\n\nimage=train_dataset[0]['image']\ndisplay(image)\nanswer=train_dataset[0]['answers']\nprint(answer)\n# If you need to ensure the image is in RGB mode\nimage = image.convert(\"RGB\")\n\ndef load_image(image):\n    return image.convert(\"RGB\")\n\ndef display_image(image):\n    display(image)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T16:24:41.259544Z","iopub.execute_input":"2024-05-18T16:24:41.259830Z","iopub.status.idle":"2024-05-18T16:24:41.265568Z","shell.execute_reply.started":"2024-05-18T16:24:41.259798Z","shell.execute_reply":"2024-05-18T16:24:41.264219Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Set up environment\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T16:24:41.267925Z","iopub.execute_input":"2024-05-18T16:24:41.268541Z","iopub.status.idle":"2024-05-18T16:24:41.385058Z","shell.execute_reply.started":"2024-05-18T16:24:41.268508Z","shell.execute_reply":"2024-05-18T16:24:41.384071Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Tokenizer and Image Processor\nbert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nimage_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T16:24:41.386256Z","iopub.execute_input":"2024-05-18T16:24:41.386838Z","iopub.status.idle":"2024-05-18T16:24:43.342045Z","shell.execute_reply.started":"2024-05-18T16:24:41.386802Z","shell.execute_reply":"2024-05-18T16:24:43.341175Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60f60f5111d34c47a3599f1c0c97631c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77db5749ec7c49968515b49bfcd8db5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aea88ff210564a6c8a2bdca9808ecba3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17f9cd7cbb7d4491ae79e10ad1e6d655"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6561ff40ea3475889ed0cf8b189fff0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"614b722106174415bde7cdc350824e36"}},"metadata":{}}]},{"cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, dataset, tokenizer, image_processor, label_encoder):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.image_processor = image_processor\n        self.label_encoder = label_encoder\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        question = item['question']\n        answers = item['answers']\n        image = item['image'].convert(\"RGB\")\n\n        text_inputs = self.tokenizer(question, padding='max_length', truncation=True, return_tensors=\"pt\")\n        image_inputs = self.image_processor(images=[image], return_tensors=\"pt\")\n\n        text_inputs = {k: v.squeeze(0) for k, v in text_inputs.items()}\n        image_inputs = {k: v.squeeze(0) for k, v in image_inputs.items()}\n\n        # Convert answers to soft targets\n        answer_texts = [answer['answer'] for answer in answers]\n        encoded_labels = self.label_encoder.transform(answer_texts)\n        one_hot_labels = np.zeros((len(self.label_encoder.classes_)))\n        for encoded_label in encoded_labels:\n            one_hot_labels[encoded_label] += 1\n        one_hot_labels /= len(encoded_labels)  # Average the one-hot vectors\n\n        label = torch.tensor(one_hot_labels, dtype=torch.float)  # Convert to tensor\n\n        return {'text_inputs': text_inputs, 'image_inputs': image_inputs, 'labels': label}\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T16:24:43.343465Z","iopub.execute_input":"2024-05-18T16:24:43.344149Z","iopub.status.idle":"2024-05-18T16:24:43.354268Z","shell.execute_reply.started":"2024-05-18T16:24:43.344110Z","shell.execute_reply":"2024-05-18T16:24:43.353198Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class VQAModel(nn.Module):\n    def __init__(self, text_model_name=\"bert-base-cased\", image_model_name=\"google/vit-base-patch16-224\", num_answers=1000):\n        super(VQAModel, self).__init__()\n        self.text_model = AutoModel.from_pretrained(text_model_name)\n        self.image_model = ViTModel.from_pretrained(image_model_name)\n        self.text_fc = nn.Linear(self.text_model.config.hidden_size, 512)\n        self.image_fc = nn.Linear(self.image_model.config.hidden_size, 512)\n        self.classifier = nn.Linear(1024, num_answers)\n        \n    def forward(self, text_inputs, image_inputs):\n        text_outputs = self.text_model(**text_inputs).last_hidden_state[:, 0, :]  # CLS token\n        image_outputs = self.image_model(**image_inputs).last_hidden_state[:, 0, :]  # CLS token\n        text_features = self.text_fc(text_outputs)\n        image_features = self.image_fc(image_outputs)\n        combined_features = torch.cat((text_features, image_features), dim=1)\n        logits = self.classifier(combined_features)\n        return logits","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\nall_answer_texts = [answer['answer'] for example in train_dataset for answer in example['answers']]\nlabel_encoder.fit(all_answer_texts)\nrandom.seed(42)\nsubset_indices = random.sample(range(len(dataset['train'])), len(dataset['train']) // 4)\nsubset_train_dataset = Subset(VQADataset(dataset['train'], bert_tokenizer, image_processor, label_encoder), subset_indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the model\nnum_answers = 162496  # Assuming a fixed number of possible answers\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = VQAModel(num_answers=num_answers).to(device)\ncriterion = nn.BCEWithLogitsLoss()  # Suitable for multi-label classification with soft targets\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, dataloader, criterion, optimizer, num_epochs=5):\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        start_time = time.time()  # start time for epoch\n        all_predictions=[]\n        all_labels=[]\n        for batch in tqdm(dataloader):\n            text_inputs = {k: v.to(device) for k, v in batch['text_inputs'].items()}\n            image_inputs = {k: v.to(device) for k, v in batch['image_inputs'].items()}\n            labels = batch['labels'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(text_inputs, image_inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n            \n        end_time = time.time()  # End time for epoch\n        epoch_time = end_time - start_time  # Time taken for epoch\n        \n        # Calculate metrics\n        accuracy = accuracy_score(np.array(all_labels), binary_predictions)\n        f1 = f1_score(all_labels, all_predictions, average='weighted')\n        binary_predictions = np.argmax(all_predictions, axis=1)\n        precision = precision_score(np.array(all_labels).argmax(axis=1), binary_predictions.argmax(axis=1), average='weighted')\n        recall = recall_score(np.array(all_labels).argmax(axis=1), binary_predictions.argmax(axis=1), average='weighted')\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader)}, '\n          f'Accuracy: {accuracy}, F1 Score: {f1}, Precision: {precision}, '\n          f'Recall: {recall}, Time Taken: {epoch_time:.2f} seconds')\n\n# DataLoader\ndataloader = DataLoader(subset_train_dataset, batch_size=2, shuffle=True)\n\n# Train the model\ntrain(model, dataloader, criterion, optimizer, num_epochs=3)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T17:34:19.026028Z","iopub.execute_input":"2024-05-18T17:34:19.027101Z","iopub.status.idle":"2024-05-18T17:34:19.034684Z","shell.execute_reply.started":"2024-05-18T17:34:19.027067Z","shell.execute_reply":"2024-05-18T17:34:19.033752Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use zero_division parameter to control this behavior._warn_prf(average, modifier, msg_start, len(result))\n\n\nEpoch 1/3, Loss: 3.66818, Accuracy: 0.224831, F1 Score: 0.003869, Precision: 0.340584, Recall: 0.219438, Time Taken: 7246.38 seconds\nEpoch 2/3, Loss: 3.29474, Accuracy: 0.252424, F1 Score: 0.010920, Precision: 0.370049, Recall: 0.221462, Time Taken: 7178.09 seconds\nEpoch 3/3, Loss: 2.99719, Accuracy: 0.265961, F1 Score: 0.012784, Precision: 0.376632, Recall: 0.238232, Time Taken: 7234.18 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the trained model\nlora_model_path = 'vqa_model_25percent.pth'\ntorch.save(lora_model.state_dict(), lora_model_path)\nprint(f'Model saved to {lora_model_path}')","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:37:42.652812Z","iopub.execute_input":"2024-05-18T17:37:42.653177Z","iopub.status.idle":"2024-05-18T17:37:42.658329Z","shell.execute_reply.started":"2024-05-18T17:37:42.653148Z","shell.execute_reply":"2024-05-18T17:37:42.657348Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Model saved to vqa_model_25percent.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_model(model_path):\n    model = VQAModel(num_answers=num_answers)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n    return model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load model\nloaded_model = load_model(model_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"download_file('/kaggle/working/vqa_model_25percent.zip', 'out')","metadata":{},"execution_count":null,"outputs":[]}]}