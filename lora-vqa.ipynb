{"cells":[{"cell_type":"code","execution_count":125,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-17T15:50:34.480876Z","iopub.status.busy":"2024-05-17T15:50:34.480155Z","iopub.status.idle":"2024-05-17T15:50:47.078010Z","shell.execute_reply":"2024-05-17T15:50:47.076721Z","shell.execute_reply.started":"2024-05-17T15:50:34.480846Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["# Install required libraries\n","!pip install transformers datasets torch peft -q\n","\n","import os\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from transformers import AutoTokenizer, AutoModel, AutoImageProcessor, ViTModel\n","from PIL import Image\n","from sklearn.preprocessing import LabelEncoder\n","from datasets import load_dataset\n","from peft import LoraConfig, get_peft_model\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n","\n","\n","from IPython.display import display\n","import random"]},{"cell_type":"code","execution_count":126,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-17T15:51:08.319645Z","iopub.status.busy":"2024-05-17T15:51:08.318812Z","iopub.status.idle":"2024-05-17T15:51:09.807349Z","shell.execute_reply":"2024-05-17T15:51:09.806546Z","shell.execute_reply.started":"2024-05-17T15:51:08.319605Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Repo card metadata block was not found. Setting CardData to empty.\n"]}],"source":["# %%time\n","# Load the VQAv2 dataset from Hugging Face Hub\n","dataset = load_dataset(\"HuggingFaceM4/VQAv2\")"]},{"cell_type":"code","execution_count":127,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:12.055690Z","iopub.status.busy":"2024-05-17T15:51:12.054878Z","iopub.status.idle":"2024-05-17T15:51:12.060187Z","shell.execute_reply":"2024-05-17T15:51:12.059181Z","shell.execute_reply.started":"2024-05-17T15:51:12.055660Z"},"trusted":true},"outputs":[],"source":["# Access train, validation, and test sets\n","train_dataset = dataset['train']\n","test_dataset = dataset['test']\n","val_dataset = dataset['validation']\n","\n","print(train_dataset[0])\n","image=train_dataset[0]['image']\n","display(image)\n","answer=train_dataset[0]['answers']\n","print(answer)\n","# If you need to ensure the image is in RGB mode\n","image = image.convert(\"RGB\")\n","\n","def load_image(image):\n","    return image.convert(\"RGB\")\n","\n","def display_image(image):\n","    display(image)"]},{"cell_type":"code","execution_count":128,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:13.584028Z","iopub.status.busy":"2024-05-17T15:51:13.583279Z","iopub.status.idle":"2024-05-17T15:51:13.973615Z","shell.execute_reply":"2024-05-17T15:51:13.972535Z","shell.execute_reply.started":"2024-05-17T15:51:13.583992Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# Tokenizer and Image Processor setup\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:15.229187Z","iopub.status.busy":"2024-05-17T15:51:15.228318Z","iopub.status.idle":"2024-05-17T15:51:15.237567Z","shell.execute_reply":"2024-05-17T15:51:15.236604Z","shell.execute_reply.started":"2024-05-17T15:51:15.229151Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# Custom Dataset class\n","class VQADataset(Dataset):\n","    def __init__(self, dataset, tokenizer, image_processor):\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.image_processor = image_processor\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        item = self.dataset[idx]\n","        question = item['question']\n","        answers = item['answers']\n","        image = item['image'].convert(\"RGB\")\n","\n","        text_inputs = self.tokenizer(question, padding='max_length', truncation=True, return_tensors=\"pt\")\n","        image_inputs = self.image_processor(images=[image], return_tensors=\"pt\")\n","\n","        text_inputs = {k: v.squeeze(0) for k, v in text_inputs.items()}\n","        image_inputs = {k: v.squeeze(0) for k, v in image_inputs.items()}\n","        \n","        answer_texts = [answer['answer'] for answer in answers]\n","        encoded_labels = self.label_encoder.transform(answer_texts)\n","        one_hot_labels = np.zeros((len(self.label_encoder.classes_)))\n","        for encoded_label in encoded_labels:\n","            one_hot_labels[encoded_label] += 1\n","        one_hot_labels /= len(encoded_labels)  # Average the one-hot vectors\n","\n","        label = torch.tensor(one_hot_labels, dtype=torch.float)  # Convert to tensor\n","\n","        return {'text_inputs': text_inputs, 'image_inputs': image_inputs, 'labels': label}"]},{"cell_type":"code","execution_count":130,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:16.735561Z","iopub.status.busy":"2024-05-17T15:51:16.735220Z","iopub.status.idle":"2024-05-17T15:51:16.741124Z","shell.execute_reply":"2024-05-17T15:51:16.740143Z","shell.execute_reply.started":"2024-05-17T15:51:16.735535Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# Data preparation\n","label_encoder = LabelEncoder()\n","all_answer_texts = [answer['answer'] for example in train_dataset for answer in example['answers']]\n","label_encoder.fit(all_answer_texts)\n","train_dataset = Subset(VQADataset(dataset['train'], bert_tokenizer, image_processor), range(len(dataset['train']) // 4))\n","dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"]},{"cell_type":"code","execution_count":131,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:18.693306Z","iopub.status.busy":"2024-05-17T15:51:18.692955Z","iopub.status.idle":"2024-05-17T15:51:18.701787Z","shell.execute_reply":"2024-05-17T15:51:18.700810Z","shell.execute_reply.started":"2024-05-17T15:51:18.693277Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# Define and modify the VQA model using LoRA\n","class VQAModel(nn.Module):\n","    def __init__(self, text_model_name=\"bert-base-cased\", image_model_name=\"google/vit-base-patch16-224\", num_answers=1000):\n","        super(VQAModel, self).__init__()\n","        self.text_model = AutoModel.from_pretrained(text_model_name)\n","        self.image_model = ViTModel.from_pretrained(image_model_name)\n","        self.text_fc = nn.Linear(self.text_model.config.hidden_size, 512)\n","        self.image_fc = nn.Linear(self.image_model.config.hidden_size, 512)\n","        self.classifier = nn.Linear(1024, num_answers)\n","\n","    def forward(self, text_inputs, image_inputs):\n","        text_outputs = self.text_model(**text_inputs).last_hidden_state[:, 0, :]  # CLS token\n","        image_outputs = self.image_model(**image_inputs).last_hidden_state[:, 0, :]  # CLS token\n","        text_features = self.text_fc(text_outputs)\n","        image_features = self.image_fc(image_outputs)\n","        combined_features = torch.cat((text_features, image_features), dim=1)\n","        logits = self.classifier(combined_features)\n","        return logits"]},{"cell_type":"code","execution_count":132,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:20.197701Z","iopub.status.busy":"2024-05-17T15:51:20.197222Z","iopub.status.idle":"2024-05-17T15:51:22.876271Z","shell.execute_reply":"2024-05-17T15:51:22.875475Z","shell.execute_reply.started":"2024-05-17T15:51:20.197669Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["# %%time\n","model = VQAModel(num_answers=162496)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)"]},{"cell_type":"code","execution_count":133,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:22.878467Z","iopub.status.busy":"2024-05-17T15:51:22.878089Z","iopub.status.idle":"2024-05-17T15:51:22.958629Z","shell.execute_reply":"2024-05-17T15:51:22.957691Z","shell.execute_reply.started":"2024-05-17T15:51:22.878433Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# LoRA Configuration\n","lora_config = LoraConfig(r=16, lora_alpha=16, target_modules=[\"query\", \"value\"], lora_dropout=0.1, bias=\"none\")\n","lora_model = get_peft_model(model, lora_config)"]},{"cell_type":"code","execution_count":134,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:25.122242Z","iopub.status.busy":"2024-05-17T15:51:25.121319Z","iopub.status.idle":"2024-05-17T15:51:25.133503Z","shell.execute_reply":"2024-05-17T15:51:25.132468Z","shell.execute_reply.started":"2024-05-17T15:51:25.122205Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# Training function with LoRA\n","def train_lora(model, dataloader, criterion, optimizer, num_epochs=5):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        start_time = time.time()  # start time for epoch\n","        all_labels = []\n","        all_predictions = []\n","        for batch in dataloader:\n","            text_inputs = {k: v.to(device) for k, v in batch['text_inputs'].items()}\n","            image_inputs = {k: v.to(device) for k, v in batch['image_inputs'].items()}\n","            labels = batch['labels'].to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(text_inputs, image_inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n","            \n","        end_time = time.time()  # End time for epoch\n","        epoch_time = end_time - start_time  # Time taken for epoch\n","        accuracy = accuracy_score(all_labels, all_predictions)\n","        f1 = f1_score(all_labels, all_predictions, average='weighted')\n","        precision = precision_score(all_labels, all_predictions, average='weighted')\n","        recall = recall_score(all_labels, all_predictions, average='weighted')\n","        \n","        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader)}, Accuracy: {accuracy}, F1 Score: {f1}, Precision: {precision}, Recall: {recall}, Time Taken: {epoch_time:.2f} seconds')\n"]},{"cell_type":"code","execution_count":135,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:51:27.318207Z","iopub.status.busy":"2024-05-17T15:51:27.317349Z","iopub.status.idle":"2024-05-17T15:51:27.328053Z","shell.execute_reply":"2024-05-17T15:51:27.327095Z","shell.execute_reply.started":"2024-05-17T15:51:27.318172Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# Setup optimizer and loss function\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(lora_model.parameters(), lr=1e-4)"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T18:03:03.926122Z","iopub.status.busy":"2024-05-18T18:03:03.925465Z","iopub.status.idle":"2024-05-18T18:03:03.931510Z","shell.execute_reply":"2024-05-18T18:03:03.930467Z","shell.execute_reply.started":"2024-05-18T18:03:03.926091Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use zero_division parameter to control this behavior._warn_prf(average, modifier, msg_start, len(result))\n","Epoch 1/3, Loss: 4.076500, Accuracy: 0.162451, F1 Score: 0.003517, Precision: 0.309171, Recall: 0.198438, Time Taken: 4389.29 seconds\n","Epoch 2/3, Loss: 3.629100, Accuracy: 0.199854, F1 Score: 0.009496, Precision: 0.348322, Recall: 0.199768, Time Taken: 4245.11 seconds\n","Epoch 3/3, Loss: 3.323100, Accuracy: 0.212109, F1 Score: 0.013184, Precision: 0.363648, Recall: 0.200010, Time Taken: 4389.58 seconds\n"]}],"source":["# Start training\n","train_lora(lora_model, dataloader, criterion, optimizer, num_epochs=3)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T18:04:32.074001Z","iopub.status.busy":"2024-05-18T18:04:32.073341Z","iopub.status.idle":"2024-05-18T18:04:32.078586Z","shell.execute_reply":"2024-05-18T18:04:32.077644Z","shell.execute_reply.started":"2024-05-18T18:04:32.073969Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved to lora_vqa_model_25percent.pth\n"]}],"source":["# %%time\n","# Save the trained model\n","lora_model_path = 'lora_vqa_model_25percent.pth'\n","torch.save(lora_model.state_dict(), lora_model_path)\n","print(f'Model saved to {lora_model_path}')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T14:45:02.579231Z","iopub.status.busy":"2024-05-17T14:45:02.578071Z","iopub.status.idle":"2024-05-17T14:46:32.405196Z","shell.execute_reply":"2024-05-17T14:46:32.404317Z","shell.execute_reply.started":"2024-05-17T14:45:02.579186Z"},"trusted":true},"outputs":[],"source":["# import shutil\n","# # Zip the model file\n","# shutil.make_archive('/kaggle/working/lora_vqa_model_25percent', 'zip', '/kaggle/working/', 'lora_vqa_model_25percent.pth')"]},{"cell_type":"code","execution_count":138,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T18:13:59.709561Z","iopub.status.busy":"2024-05-17T18:13:59.708835Z","iopub.status.idle":"2024-05-17T18:13:59.716014Z","shell.execute_reply":"2024-05-17T18:13:59.715000Z","shell.execute_reply.started":"2024-05-17T18:13:59.709530Z"},"trusted":true},"outputs":[],"source":["import os\n","import subprocess\n","from IPython.display import FileLink, display\n","\n","def download_file(path, download_file_name):\n","    os.chdir('/kaggle/working/')\n","    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n","    command = f\"zip {zip_name} {path} -r\"\n","    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n","    if result.returncode != 0:\n","        print(\"Unable to run zip command!\")\n","        print(result.stderr)\n","        return\n","    display(FileLink(f'{download_file_name}.zip'))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["download_file('/kaggle/working/lora_vqa_model_25percent.pth', 'out')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-17T15:24:07.797926Z","iopub.status.busy":"2024-05-17T15:24:07.797535Z","iopub.status.idle":"2024-05-17T15:24:09.086512Z","shell.execute_reply":"2024-05-17T15:24:09.085489Z","shell.execute_reply.started":"2024-05-17T15:24:07.797892Z"},"trusted":true},"outputs":[],"source":["# !rm -rf /kaggle/working/out*"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
